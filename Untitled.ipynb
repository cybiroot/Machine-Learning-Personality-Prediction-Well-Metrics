{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a6e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing tools\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "str_punc = string.punctuation\n",
    "\n",
    "engstopwords = stopwords.words(\"english\")\n",
    "engstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n",
    "                        ' '.join(engstopwords)).split()\n",
    "\n",
    "engstopwords = set(engstopwords).union(set(engstopwordsV2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40a3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie pentru a clasifica un cuvant ca adjectiv, verb, substantiv\n",
    "def lemmatize_all_types(word):\n",
    "    word = wnl.lemmatize(word, 'a')\n",
    "    word = wnl.lemmatize(word, 'v')\n",
    "    word = wnl.lemmatize(word, 'n')\n",
    "    return word\n",
    "\n",
    "# Functie pentru a \"curata\" textul de link-uri, caractere speciale etc\n",
    "def clean(text):\n",
    "    # sterge URL-urile din text\n",
    "    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n",
    "    url_regex = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    text = re.sub(url_regex, \"\", text)\n",
    "\n",
    "    # sterge semne de punctuatie\n",
    "    text = re.sub(r'(:|;).', \" \", text)\n",
    "    text = re.sub('['+re.escape(str_punc)+']',\" \",  text)\n",
    "    \n",
    "    # sterge paranteze\n",
    "    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', ' ', text)\n",
    "    text = re.sub('[’‘“\\.”…–]', '', text)\n",
    "    text = re.sub('[^(\\w|\\s)]', '', text)\n",
    "    text = re.sub('(gt|lt)', '', text)\n",
    "    \n",
    "    # verifica daca exista stopwords, si il clasifica\n",
    "    text = list(map(lemmatize_all_types, text.split()))\n",
    "    text = [word for word in text if (word not in engstopwords)]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# convertiti tipul de personalitate in functia cognitiva dominanta\n",
    "def letters_to_functions(personality_type):\n",
    "    translator = {\n",
    "            'ENxP': 'Ne',\n",
    "            'INxJ': 'Ni',\n",
    "            'ESxP': 'Se',\n",
    "            'ISxJ': 'Si',\n",
    "            'ExTJ': 'Te',\n",
    "            'IxTP': 'Ti',\n",
    "            'IxFP': 'Fi',\n",
    "            'ExFJ': 'Fe',\n",
    "            \n",
    "            'xNxx': 'N',\n",
    "            'xSxx': 'S',\n",
    "            'xxTx' : 'T',\n",
    "            'xxFx': 'F',\n",
    "            'Ixxx':'I',\n",
    "            'Exxx':'E',\n",
    "            }\n",
    "    return translator[personality_type]\n",
    "\n",
    "# convertiti tipul de functie cognitiva dominanta in tip de personalitate\n",
    "def functions_to_letters(functions):\n",
    "    translator = {\n",
    "            'NiTe': 'INTJ',\n",
    "            'NiFe': 'INFJ',\n",
    "            'NeTi': 'ENTP',\n",
    "            'NeFi': 'ENFP',\n",
    "            'SiTe': 'ISTJ',\n",
    "            'SiFe': 'ISFJ',\n",
    "            'SeTi': 'ESTP',\n",
    "            'SeFi': 'ESFP',\n",
    "            'TeNi': 'ENTJ',\n",
    "            'FeNi': 'ENFJ',\n",
    "            'TiNe': 'INTP',\n",
    "            'FiNe': 'INFP',\n",
    "            'TeSi': 'ESTJ',\n",
    "            'FeSi': 'ESFJ',\n",
    "            'TiSe': 'ISTP',\n",
    "            'FiSe': 'ISFP',\n",
    "            }\n",
    "    return translator[functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3662570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functiile cognitive descrise\n",
    "cf_info = {\n",
    "        \"Ni\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Ne\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        \"Si\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Se\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Ti': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        'Te': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Fi': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'internal',\n",
    "                },\n",
    "        'Fe': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                }\n",
    "        }\n",
    "\n",
    "# dictionarul modelurilor \"pretrained\"\n",
    "models = {\n",
    "        \n",
    "        'NiNe':None,\n",
    "        'NiSi':None,\n",
    "        'NiSe':None,\n",
    "        'NeSi':None,\n",
    "        'NeSe':None,\n",
    "        'SiSe':None,\n",
    "        \n",
    "        'TiTe':None,\n",
    "        'TiFi':None,\n",
    "        'TiFe':None,\n",
    "        'TeFi':None,\n",
    "        'TeFe':None,\n",
    "        'FiFe':None,\n",
    "            \n",
    "        'NiTe':None,\n",
    "        'NiFe':None,\n",
    "        'SiTe':None,\n",
    "        'SiFe':None,\n",
    "        \n",
    "        'NeFi':None,\n",
    "        'NeTi':None,\n",
    "        'SeFi':None,\n",
    "        'SeTi':None,\n",
    "        \n",
    "        'NiTi':None,\n",
    "        'NiFi':None,\n",
    "        'NeTe':None,\n",
    "        'NeFe':None,\n",
    "        \n",
    "        'SiTi':None,\n",
    "        'SiFi':None,\n",
    "        'SeTe':None,\n",
    "        'SeFe':None,\n",
    "        }\n",
    "\n",
    "# modele pretrained (cresc acuratetea cu ~3%)\n",
    "supporters = {\n",
    "        'NvS':None,\n",
    "        'FvT':None,\n",
    "        'IvE':None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citeste modelele\n",
    "path = \"mbti-pretrained-models/\"\n",
    "\n",
    "for key in models:\n",
    "    with open(path + key + '.pickle', 'rb') as file:\n",
    "        models[key] = pickle.load(file)\n",
    "\n",
    "for _key in supporters:\n",
    "    with open(path + _key + '.pickle', 'rb') as file:\n",
    "        supporters[_key] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925cfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertirea unei functii cognitive in alta f.c. cu directie diferita (e -> i si vice versa)\n",
    "def flip_cf_direction(cognitiveFunction):\n",
    "    direction = cognitiveFunction[1]\n",
    "    new_direction = 'i' if direction == 'e' else 'e'\n",
    "    return cognitiveFunction[0] + new_direction\n",
    "\n",
    "# folosirea unui model pre antrenat pentru o \"proba\"\n",
    "def process_classify_sample(modelObject, sample):\n",
    "    vectorizer = modelObject['cv']\n",
    "    label_encoder = modelObject['labelEncoder']\n",
    "    model = modelObject['model']\n",
    "    \n",
    "    # preprocessing\n",
    "    clean_sample = clean(sample)\n",
    "    x = vectorizer.transform([clean_sample]).toarray()\n",
    "    \n",
    "    # clasificare\n",
    "    y = model.predict(x)\n",
    "    y_probability = max(model.predict_proba(x)[0])\n",
    "    classified_cf = label_encoder.inverse_transform(y)[0]\n",
    "    return letters_to_functions(classified_cf), y_probability\n",
    "\n",
    "    \n",
    "# Prima faza pentru clasificarea unui tip de personalitate\n",
    "# mai exact functia cognitiva perceptoare (P)\n",
    "# si functia functia cognitiva de gandire (J)\n",
    "def phase1(sample):\n",
    "    \n",
    "    # Keep track of every input (perceiving) cognitive function likelihood\n",
    "    input_cf_acc = pd.Series({\"Ni\":0, \"Ne\":0, \"Si\":0, \"Se\":0}, dtype=float)\n",
    "    # same for output (judging) cognitive functions\n",
    "    output_cf_acc = pd.Series({\"Ti\":0, \"Te\":0, \"Fi\":0, \"Fe\":0}, dtype=float)\n",
    "    \n",
    "    input_cf = np.array(['Ni', 'Ne', 'Si', 'Se'])\n",
    "    output_cf = np.array(['Ti', 'Te', 'Fi', 'Fe'])\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = input_cf[i] + input_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # creste precizia clasei de prezicere\n",
    "            input_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = input_cf[i] if input_cf[j] == cognitive_fn else input_cf[j]\n",
    "            input_cf_acc[other_cf] += 1 - probability\n",
    "            \n",
    "    # Another nested loop for output (judging) cognitive functions\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = output_cf[i] + output_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # creste precizia clasei de prezicere\n",
    "            output_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = output_cf[i] if output_cf[j] == cognitive_fn else output_cf[j]\n",
    "            output_cf_acc[other_cf] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (iNtuition vs Sensing)\n",
    "    modelObject = supporters[\"NvS\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase iNtuitive functions\n",
    "    if cognitive_fn == \"N\":\n",
    "        input_cf_acc[['Ni', 'Ne']] += probability\n",
    "        input_cf_acc[['Si', 'Se']] += 1 - probability\n",
    "    # Increase Sensing functions\n",
    "    else:\n",
    "        input_cf_acc[['Si', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Ne']] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (Feeling vs Thinking)\n",
    "    modelObject = supporters[\"FvT\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Feeling functions likelihood\n",
    "    if cognitive_fn == \"F\":\n",
    "        output_cf_acc[['Fi', 'Fe']] += probability\n",
    "        output_cf_acc[['Ti', 'Te']] += 1 - probability\n",
    "    # Increase Thinking Functions likelihood\n",
    "    else:\n",
    "        output_cf_acc[['Ti', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Fe']] += 1 - probability   \n",
    "    \n",
    "    # Use supporter model (Introvert vs Extrovert)\n",
    "    modelObject = supporters[\"IvE\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Introverted functions likelihood\n",
    "    if cognitive_fn == \"I\":\n",
    "        input_cf_acc[['Ni', 'Si']] += probability\n",
    "        input_cf_acc[['Ne', 'Se']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fi', 'Ti']] += probability\n",
    "        output_cf_acc[['Fe', 'Te']] += 1 - probability\n",
    "        \n",
    "    # Increase  Extroverted functions likelihood\n",
    "    else:\n",
    "        input_cf_acc[['Ne', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Si']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fe', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Ti']] += 1 - probability\n",
    "    \n",
    "    # Return: likelihoods of perceiving (input) cognitive functions\n",
    "    # and judging (output) cogitive function\n",
    "    return input_cf_acc, output_cf_acc\n",
    "\n",
    "    \n",
    "    \n",
    "# Phase 2 of classification algorithm\n",
    "# Determine which cognitive function is the dominant\n",
    "# and which is the auxiliary.\n",
    "# and Fix the classification if necessary\n",
    "# Necessary: if phase 1 results a two cognitive functions\n",
    "# of the same direction (ex: Ni-Ti), this is not acceptable\n",
    "# since there's no personality with these Dom-Aux functions\n",
    "def phase2(sample, input_acc, output_acc):\n",
    "    # Number of classifications done on every cognitive functions\n",
    "    # (i.e. we ran 5 models having 'Ni' as one of it's classes)\n",
    "    counter_models_ran = 5\n",
    "    \n",
    "    # Get max-likelihood data (probability & className)\n",
    "    # maxInput is the perceiving function which got the maximum likelihood\n",
    "    # maxOutput is the judgning function ~~~~~~\n",
    "    maxInput = {'name':input_acc.idxmax(), 'proba':input_acc.max()}\n",
    "    maxOutput = {'name':output_acc.idxmax(), 'proba':output_acc.max()}\n",
    "    \n",
    "    # Get direction of each cognitive function\n",
    "    maxInputDirection = cf_info[maxInput['name']]['direction']\n",
    "    maxOutputDirection = cf_info[maxOutput['name']]['direction']\n",
    "    \n",
    "    # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "    cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "    phase2_model_name = \"\".join(cf_stack)\n",
    "    \n",
    "    # if both perceiving & judging classes (functions) are opposite direction\n",
    "    if maxInputDirection != maxOutputDirection:\n",
    "        # We know the top two cognitive functions (ie. Ni, Te)\n",
    "        # this path will run the appropriate models to\n",
    "        # determine which of these functions is Dominant (primary)\n",
    "        # and which is Auxiliary (secondary)\n",
    "\n",
    "        # determine which cognitive_function is the dominant one\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "    # both perceiving & judging functions have same direction (they must NOT)\n",
    "    else:\n",
    "        # There is an ambiguity, the top 2 cognitive_functions cannot be of\n",
    "        # the same direction (ie. Ni, Ti) (they're both 'i')\n",
    "        # One of them is correct, this function will detect which one is\n",
    "        \n",
    "        # Detect which of these functions is more accurate\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "        \n",
    "        # if dominant cognitive function is an input (perceiving) \n",
    "        # (ie: Ni, Ne, Si, Se)\n",
    "        # flip the direction of the other -output- function\n",
    "        # example: Ni-Ti  => Ni-Te\n",
    "        if dominant_cf_name == input_acc.idxmax():\n",
    "            problematic_cf = output_acc.idxmax()\n",
    "            input_acc[input_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = output_acc[fixed_cf_name]\n",
    "            output_acc[fixed_cf_name] = output_acc[problematic_cf] + (1 - probability)\n",
    "            output_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # else if the dominant cognitive function is an output (judging)\n",
    "        # (ie: Ti, Te, Fi, Fe)\n",
    "        # flip the direction of the other -input- function\n",
    "        # example: Ni-Ti => Ne-Ti\n",
    "        else:\n",
    "            problematic_cf = input_acc.idxmax()\n",
    "            output_acc[output_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = input_acc[fixed_cf_name]\n",
    "            input_acc[fixed_cf_name] = input_acc[problematic_cf] + (1 - probability)\n",
    "            input_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # Now we've fixed the direction of the (less) accurate cognitive function\n",
    "        # We need to determine which function is dominant\n",
    "        \n",
    "        # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "        cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "        phase2_final_model_name = \"\".join(cf_stack)\n",
    "\n",
    "        # Run model\n",
    "        modelObject = models[phase2_final_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "    \n",
    "    # Now we know which cognitive function is dominant and which is auxilary.\n",
    "    # handle their likelihoods\n",
    "    \n",
    "    # If dominant function is input (perceiving)\n",
    "    # increase its likelihood, decrease the other\n",
    "    if dominant_cf_name == input_acc.idxmax():\n",
    "        input_acc[input_acc.idxmax()] += probability\n",
    "        output_acc[output_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # else, dominant function is output (judging)\n",
    "    # increase its likelihood, decrease the other\n",
    "    else:\n",
    "        output_acc[output_acc.idxmax()] += probability\n",
    "        input_acc[input_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # Select the winners in phase 1\n",
    "    cognitive_functions_stack = pd.Series({\n",
    "            input_acc.idxmax(): input_acc[input_acc.idxmax()],\n",
    "            output_acc.idxmax(): output_acc[output_acc.idxmax()]\n",
    "            })\n",
    "    \n",
    "    # Select the dominant function as the 1st winner in phase 2\n",
    "    dominant_function =  dominant_cf_name\n",
    "    \n",
    "    # Select the auxiliary function as the 2nd winner in phase 2\n",
    "    auxiliary_function = cognitive_functions_stack.drop(dominant_cf_name).index[0]\n",
    "\n",
    "    # converteste functiile cognitive in tipuri de personalitate\n",
    "    personality_type = functions_to_letters(dominant_function + auxiliary_function)\n",
    "    \n",
    "    # calculeaza probabilitatea\n",
    "    probability = (cognitive_functions_stack[dominant_function] / counter_models_ran \n",
    "                   + cognitive_functions_stack[auxiliary_function] / counter_models_ran) / 2\n",
    "    \n",
    "    return personality_type, probability\n",
    "\n",
    "# aplica clasificarea\n",
    "def run(sample):\n",
    "    input_acc, output_acc = phase1(sample)\n",
    "    personality, probability = phase2(sample, input_acc, output_acc)\n",
    "    return personality, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff300763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDARE\n",
    "validation_set = pd.read_csv(path + 'validation_set.csv')\n",
    "\n",
    "def classify(sentence):\n",
    "    personality, probability = run(sentence)\n",
    "    return personality\n",
    "\n",
    "# rezultate - 16 tipuri de personalitate\n",
    "y_pred = validation_set['posts'].apply(classify) # dureaza aproximativ 5 minute\n",
    "y_true = validation_set['type']\n",
    "\n",
    "# rezultate - 4 categorii\n",
    "y_pred_soft = y_pred.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')\n",
    "y_true_soft = y_true.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be51083",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Classes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NF       0.84      0.83      0.84       649\n",
      "          NT       0.93      0.86      0.89      1202\n",
      "          SF       0.29      0.66      0.40        32\n",
      "          ST       0.54      0.77      0.63       117\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.65      0.78      0.69      2000\n",
      "weighted avg       0.87      0.84      0.85      2000\n",
      "\n",
      "\n",
      "16 Classes\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.00      0.00      0.00         0\n",
      "        ENFP       0.54      0.70      0.61        74\n",
      "        ENTJ       0.00      0.00      0.00         0\n",
      "        ENTP       0.64      0.66      0.65       137\n",
      "        ESFJ       0.00      0.00      0.00         0\n",
      "        ESFP       0.25      0.33      0.29         3\n",
      "        ESTJ       0.00      0.00      0.00         0\n",
      "        ESTP       0.58      0.97      0.73        29\n",
      "        INFJ       0.79      0.67      0.72       310\n",
      "        INFP       0.78      0.75      0.76       265\n",
      "        INTJ       0.84      0.71      0.77       502\n",
      "        INTP       0.86      0.82      0.84       563\n",
      "        ISFJ       0.25      0.67      0.36        12\n",
      "        ISFP       0.20      0.35      0.26        17\n",
      "        ISTJ       0.27      0.65      0.38        17\n",
      "        ISTP       0.49      0.54      0.51        71\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.41      0.49      0.43      2000\n",
      "weighted avg       0.77      0.73      0.74      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reporturile clasificarii\n",
    "print(\"4 Classes\")\n",
    "print(classification_report(y_true_soft, y_pred_soft))\n",
    "\n",
    "print(\"\\n16 Classes\\n\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636506e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ESTP\n",
      "Likelihood: 0.664963411206571\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "basketball and mathematics\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ENTJ\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5b189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ISFJ\n",
      "Likelihood: 0.6364602314924477\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "I love cars, I enjoy listening rap, and rock, I can't help but eat pizza every friday night, alone.\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ISFJ\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f93e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ESFJ\n",
      "Likelihood: 0.6078334353728772\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "very talkative person, I enjoy telling stories and communicating with others, I never skip gym classes and I love informatics. \n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ESFJ\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094569b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
